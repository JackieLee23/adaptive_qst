{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f1214c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import qiskit\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, A2C, DQN, TD3, SAC\n",
    "from stable_baselines3.common.env_util import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "from qiskit.quantum_info import random_density_matrix, random_statevector, DensityMatrix\n",
    "from adaptive_qst.plotting import PlotOneQubit\n",
    "from adaptive_qst.max_info import Posterior, HiddenState\n",
    "#from adaptive_qst.rl_qst import AQSTEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import pi\n",
    "from qiskit.quantum_info import state_fidelity\n",
    "\n",
    "from numpy import sqrt\n",
    "from numpy.linalg import cholesky\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d0f4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAQST(gym.Env):\n",
    "    \n",
    "    def __init__(self, n_particles = 30, n_measurements = 1000, hidden_state = None, reward_start = 100):\n",
    "        super(RAQST, self).__init__()\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.n_measurements = n_measurements\n",
    "        self.posterior = Posterior(self.n_particles)\n",
    "        \n",
    "        self.change_state = (hidden_state is None)\n",
    "        self.hidden_state = hidden_state\n",
    "        if self.change_state:\n",
    "            self.hidden_state = HiddenState()\n",
    "    \n",
    "        self.step_num = 0\n",
    "        self.reward_start = reward_start\n",
    "        \n",
    "        ##Observations: Orientation of the measured axis\n",
    "        self.observation_space = gym.spaces.Box(low = -1, high = 1, shape = (2,))\n",
    "        \n",
    "        ##Actions: Orientation of the measured axis\n",
    "        self.action_space = gym.spaces.Box(low= -1, high = 1, shape = (2,))  ##Orientation of measurement\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        config = (action + 1) * pi / 2\n",
    "        result = self.hidden_state.measure_along_axis(config)\n",
    "        \n",
    "        \n",
    "        self.posterior.update(config, result)\n",
    "        fidelity = state_fidelity(self.hidden_state.hidden_state, self.posterior.get_best_guess())\n",
    "        \n",
    "        reward = -np.log(1 - fidelity) / np.log(self.step_num + 8)\n",
    "\n",
    "        self.step_num += 1\n",
    "        truncated = (self.step_num >= self.n_measurements)\n",
    "        terminated = False\n",
    "\n",
    "        return (self.get_observation(config, result), \n",
    "                reward, \n",
    "                terminated, \n",
    "                truncated, \n",
    "                {})\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "\n",
    "        self.posterior = Posterior(self.n_particles)\n",
    "        if not self.change_state:\n",
    "            self.hidden_state = HiddenState()\n",
    "            \n",
    "        self.step_num = 0\n",
    "        \n",
    "        return np.array([0, 0]).astype(np.float32), {}\n",
    "    \n",
    "    #Package complex density matrix and weights into observation vector (x, y, z positions)\n",
    "    def get_observation(self, config, result):\n",
    "        \n",
    "        if result == 0:\n",
    "            obs = config\n",
    "        \n",
    "        ##Otherwise, measured in the opposite direction:\n",
    "        else:\n",
    "            obs = np.array([pi - config[0], config[1] - pi])\n",
    "            \n",
    "        #Normalize\n",
    "        obs /= pi\n",
    "        obs[0] = 2 * obs[0] - 1\n",
    "\n",
    "        return obs.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cb991fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RAQST()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "957bef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37932843 0.6402268 ]\n",
      "[0.3793285  0.82011336]\n",
      "\n",
      "[-0.4464065  0.1054719]\n",
      "[ 0.44640645 -0.44726405]\n",
      "\n",
      "[ 0.49388504 -0.3730353 ]\n",
      "[0.49388492 0.31348234]\n",
      "\n",
      "[ 0.7541785 -0.7234208]\n",
      "[0.7541784 0.1382896]\n",
      "\n",
      "[0.15258603 0.17683956]\n",
      "[0.15258598 0.5884198 ]\n",
      "\n",
      "[0.92862636 0.00646803]\n",
      "[-0.9286264  -0.49676597]\n",
      "\n",
      "[0.75920683 0.97662836]\n",
      "[0.75920665 0.98831415]\n",
      "\n",
      "[ 0.4609135 -0.4513865]\n",
      "[-0.46091357 -0.7256932 ]\n",
      "\n",
      "[-0.58582515 -0.5354774 ]\n",
      "[-0.58582515  0.2322613 ]\n",
      "\n",
      "[0.21020266 0.6387054 ]\n",
      "[0.2102027 0.8193527]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Test the Environment with random actions:\n",
    "env = RAQST()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, rewards, truncated, terminated, info = env.step(action)\n",
    "    \n",
    "    print(action)\n",
    "    print(obs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae0bc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"models/lstm_qst\"\n",
    "tb_log_dir = \"tb_logs/lstm_qst/1_qubit\"\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok = True)\n",
    "os.makedirs(tb_log_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f602c98",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# model = RecurrentPPO(\"MlpLstmPolicy\", \"CartPole-v1\", verbose=1)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m RecurrentPPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpLstmPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_env, tensorboard_log \u001b[38;5;241m=\u001b[39m tb_log_dir)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_measurements\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_train_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\measurement-feedback-project\\lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:469\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    466\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 469\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\measurement-feedback-project\\lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:260\u001b[0m, in \u001b[0;36mRecurrentPPO.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[0;32m    259\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\measurement-feedback-project\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:104\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\measurement-feedback-project\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:449\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 449\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluations_timesteps\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\measurement-feedback-project\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:94\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     89\u001b[0m         observations,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     90\u001b[0m         state\u001b[38;5;241m=\u001b[39mstates,\n\u001b[0;32m     91\u001b[0m         episode_start\u001b[38;5;241m=\u001b[39mepisode_starts,\n\u001b[0;32m     92\u001b[0m         deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[0;32m     96\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\measurement-feedback-project\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\measurement-feedback-project\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\measurement-feedback-project\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[1;32mIn[26], line 30\u001b[0m, in \u001b[0;36mRAQST.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     26\u001b[0m config \u001b[38;5;241m=\u001b[39m (action \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m pi \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     27\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state\u001b[38;5;241m.\u001b[39mmeasure_along_axis(config)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m fidelity \u001b[38;5;241m=\u001b[39m state_fidelity(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state\u001b[38;5;241m.\u001b[39mhidden_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposterior\u001b[38;5;241m.\u001b[39mget_best_guess())\n\u001b[0;32m     33\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m fidelity) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\jackson\\measurement_feedback_project\\adaptive_qst\\src\\adaptive_qst\\max_info.py:178\u001b[0m, in \u001b[0;36mPosterior.update\u001b[1;34m(self, config, result)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_arr\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_effective_sample_size() \u001b[38;5;241m<\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresampling_tolerance \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_particles):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m#print(f\"Resampling with MC\")\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\jackson\\measurement_feedback_project\\adaptive_qst\\src\\adaptive_qst\\max_info.py:166\u001b[0m, in \u001b[0;36mPosterior.resample\u001b[1;34m(self, n_steps)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpurified_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_purified_states(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparticle_states)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_likelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_log_likelihood(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparticle_states)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmc_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weight_sequence()\n",
      "File \u001b[1;32mc:\\users\\jackson\\measurement_feedback_project\\adaptive_qst\\src\\adaptive_qst\\max_info.py:144\u001b[0m, in \u001b[0;36mPosterior.mc_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmc_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    143\u001b[0m     next_pure, next_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_next_pos()\n\u001b[1;32m--> 144\u001b[0m     next_log_likelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_full_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     acceptance_ratios \u001b[38;5;241m=\u001b[39m next_log_likelihoods \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_likelihoods\n\u001b[0;32m    147\u001b[0m     accepted \u001b[38;5;241m=\u001b[39m (log(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_particles)) \u001b[38;5;241m<\u001b[39m acceptance_ratios)\n",
      "File \u001b[1;32mc:\\users\\jackson\\measurement_feedback_project\\adaptive_qst\\src\\adaptive_qst\\max_info.py:114\u001b[0m, in \u001b[0;36mPosterior.get_full_log_likelihood\u001b[1;34m(self, particle_states)\u001b[0m\n\u001b[0;32m    111\u001b[0m log_likelihoods \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_particles)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m configuration, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_arr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_arr):\n\u001b[1;32m--> 114\u001b[0m     log_likelihoods \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparticle_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_likelihoods\n",
      "File \u001b[1;32mc:\\users\\jackson\\measurement_feedback_project\\adaptive_qst\\src\\adaptive_qst\\max_info.py:51\u001b[0m, in \u001b[0;36mPosterior.get_likelihood\u001b[1;34m(self, configuration, result, particle_states)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_likelihood\u001b[39m(\u001b[38;5;28mself\u001b[39m, configuration, result, particle_states):\n\u001b[1;32m---> 51\u001b[0m     orient_unitary \u001b[38;5;241m=\u001b[39m \u001b[43morient_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     rotated_states \u001b[38;5;241m=\u001b[39m orient_unitary \u001b[38;5;241m@\u001b[39m particle_states \u001b[38;5;241m@\u001b[39m get_adjoint(orient_unitary)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mreal(rotated_states[:, result, result])\n",
      "File \u001b[1;32mc:\\users\\jackson\\measurement_feedback_project\\adaptive_qst\\src\\adaptive_qst\\max_info.py:21\u001b[0m, in \u001b[0;36morient_measure\u001b[1;34m(configuration)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21morient_measure\u001b[39m(configuration):\n\u001b[0;32m     20\u001b[0m     theta, phi \u001b[38;5;241m=\u001b[39m configuration\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([[\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m, sin(theta\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39mexp(\u001b[38;5;241m-\u001b[39mphi \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39mj)],\n\u001b[0;32m     22\u001b[0m                      [\u001b[38;5;241m-\u001b[39msin(theta\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), cos(theta\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39mexp(\u001b[38;5;241m-\u001b[39mphi \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39mj)]])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_name = \"PPO_default\"\n",
    "model_save_path = f\"{model_save_dir}/{run_name}\"\n",
    "\n",
    "batch_size = 32\n",
    "n_measurements = 1000\n",
    "n_train_episodes = 10\n",
    "eval_episode_freq = 1\n",
    "n_eval_episodes = 10\n",
    "\n",
    "train_env = DummyVecEnv([lambda: Monitor(RAQST(n_measurements = n_measurements)) for _ in range(batch_size)])\n",
    "#train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: Monitor(RAQST(n_measurements = n_measurements))])\n",
    "#eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=model_save_path,\n",
    "                             deterministic=True, render=False, n_eval_episodes = n_eval_episodes,\n",
    "                             eval_freq = eval_episode_freq * n_measurements)\n",
    "\n",
    "\n",
    "# model = RecurrentPPO(\"MlpLstmPolicy\", \"CartPole-v1\", verbose=1)\n",
    "\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", train_env, tensorboard_log = tb_log_dir)\n",
    "model.learn(batch_size * n_measurements * n_train_episodes, tb_log_name=run_name, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0833f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.3609309e-04 -8.0317521e-05]]\n",
      "[[ 0.00027938 -0.00026112]]\n",
      "[[ 0.00036952 -0.00035382]]\n",
      "[[ 0.00042778 -0.00039793]]\n",
      "[[ 0.00046694 -0.00041662]]\n",
      "[[ 0.0004484  -0.00013724]]\n",
      "[[4.631229e-04 2.123799e-05]]\n",
      "[[0.0004875  0.00011202]]\n",
      "[[0.0005083  0.00016208]]\n",
      "[[ 5.654755e-04 -9.257115e-05]]\n",
      "[[ 0.00057907 -0.00023797]]\n",
      "[[ 0.00057369 -0.00032093]]\n",
      "[[ 0.00056534 -0.00036646]]\n",
      "[[ 0.00055851 -0.0003909 ]]\n",
      "[[ 0.0005085  -0.00011904]]\n",
      "[[ 0.00054603 -0.00025027]]\n",
      "[[ 5.151750e-04 -3.922511e-05]]\n",
      "[[ 0.00055505 -0.00020613]]\n",
      "[[ 5.2276300e-04 -1.5825279e-05]]\n",
      "[[5.1656814e-04 8.9213645e-05]]\n",
      "[[0.00052323 0.00014943]]\n",
      "[[0.00053125 0.00018251]]\n",
      "[[ 5.795658e-04 -8.094103e-05]]\n",
      "[[ 0.00058766 -0.00023133]]\n",
      "[[ 0.00057888 -0.00031713]]\n",
      "[[ 5.2402326e-04 -8.0160062e-05]]\n",
      "[[5.122069e-04 5.369253e-05]]\n",
      "[[ 0.00056172 -0.00015171]]\n",
      "[[5.3061242e-04 1.4013854e-05]]\n",
      "[[ 0.00056579 -0.00017721]]\n",
      "[[ 0.00057425 -0.00028414]]\n",
      "[[ 5.2448490e-04 -6.1479535e-05]]\n",
      "[[ 0.00055732 -0.00021922]]\n",
      "[[ 5.2304112e-04 -2.3022756e-05]]\n",
      "[[5.1629316e-04 8.5449843e-05]]\n",
      "[[ 0.00056575 -0.00013451]]\n",
      "[[5.3374661e-04 2.3007939e-05]]\n",
      "[[ 0.00056779 -0.00017247]]\n",
      "[[ 0.00057549 -0.00028163]]\n",
      "[[ 5.2530773e-04 -6.0177248e-05]]\n",
      "[[ 0.00055779 -0.0002185 ]]\n",
      "[[ 0.00056771 -0.00030641]]\n",
      "[[ 0.00056449 -0.0003574 ]]\n",
      "[[ 0.00051392 -0.00010113]]\n",
      "[[ 0.00054977 -0.00024073]]\n",
      "[[ 0.00056248 -0.00031837]]\n",
      "[[ 5.161559e-04 -7.935518e-05]]\n",
      "[[ 0.00055233 -0.0002289 ]]\n",
      "[[ 5.1977730e-04 -2.8067792e-05]]\n",
      "[[ 0.0005579  -0.00020021]]\n",
      "[[ 5.2464561e-04 -1.2751686e-05]]\n",
      "[[5.177464e-04 9.082805e-05]]\n",
      "[[ 0.00056675 -0.00013172]]\n",
      "[[ 0.00057822 -0.00025872]]\n",
      "[[ 5.2832003e-04 -4.7872803e-05]]\n",
      "[[ 0.00055999 -0.00021194]]\n",
      "[[ 0.00056916 -0.00030294]]\n",
      "[[ 0.00056539 -0.00035556]]\n",
      "[[ 0.00051452 -0.00010017]]\n",
      "[[5.060705e-04 4.326761e-05]]\n",
      "[[0.00051487 0.00012505]]\n",
      "[[0.00052547 0.00016976]]\n",
      "[[0.00053384 0.00019354]]\n",
      "[[ 5.8148836e-04 -7.5014796e-05]]\n",
      "[[ 0.00058894 -0.0002282 ]]\n",
      "[[ 5.3588074e-04 -3.2004442e-05]]\n",
      "[[5.214665e-04 7.920411e-05]]\n",
      "[[0.00052502 0.00014386]]\n",
      "[[0.00053188 0.00017961]]\n",
      "[[ 5.797200e-04 -8.228943e-05]]\n",
      "[[5.4452365e-04 5.0601528e-05]]\n",
      "[[ 0.00057457 -0.00015758]]\n",
      "[[ 0.00057967 -0.00027362]]\n",
      "[[ 5.280520e-04 -5.603506e-05]]\n",
      "[[ 0.00055934 -0.00021618]]\n",
      "[[ 0.00056862 -0.0003051 ]]\n",
      "[[ 0.00056501 -0.00035666]]\n",
      "[[ 0.00051425 -0.00010075]]\n",
      "[[5.0589442e-04 4.2965243e-05]]\n",
      "[[0.00051475 0.00012489]]\n",
      "[[0.0005254  0.00016967]]\n",
      "[[ 5.760492e-04 -8.785079e-05]]\n",
      "[[ 0.00058554 -0.00023506]]\n",
      "[[ 5.336341e-04 -3.555228e-05]]\n",
      "[[5.200693e-04 7.734392e-05]]\n",
      "[[ 0.00056685 -0.00013908]]\n",
      "[[5.3408527e-04 2.0554889e-05]]\n",
      "[[0.00052489 0.00010864]]\n",
      "[[ 0.00057123 -0.00012198]]\n",
      "[[5.3741777e-04 2.9512350e-05]]\n",
      "[[ 0.00056991 -0.0001689 ]]\n",
      "[[5.3295208e-04 3.5917728e-06]]\n",
      "[[ 0.00056605 -0.00018302]]\n",
      "[[ 0.00057401 -0.0002872 ]]\n",
      "[[ 5.242143e-04 -6.306080e-05]]\n",
      "[[5.135349e-04 6.301824e-05]]\n",
      "[[0.00051993 0.00013544]]\n",
      "[[ 0.00057121 -0.00010639]]\n",
      "[[5.385209e-04 3.801967e-05]]\n",
      "[[0.00052822 0.00011777]]\n"
     ]
    }
   ],
   "source": [
    "##Load best model and evaluate\n",
    "model = RecurrentPPO.load(f\"{model_save_path}/best_model\")\n",
    "\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", train_env, tensorboard_log = tb_log_dir)\n",
    "\n",
    "obs = eval_env.reset()\n",
    "# cell and hidden state of the LSTM\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "\n",
    "for i in range(100):\n",
    "    action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "    obs, rewards, dones, info = eval_env.step(action)\n",
    "    episode_starts = dones\n",
    "\n",
    "    print(action)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
